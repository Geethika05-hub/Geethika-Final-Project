
# Auto Interview Pro

---

## ðŸ“Œ Project Overview
Auto Interview Pro is a modular, Flask-based Generative AI application that automates interview preparation by dynamically generating technical questions, behavioral questions, interview tips, and resume suggestions based on user inputs (topics or job descriptions).  
It uses a local LLM server (Ollama) to ensure security and fast response times, achieving high levels of SDLC automation without reliance on cloud services.

---

## ðŸŽ¯ Core Features
- ðŸ”¹ **Automated Interview Question Generation** (Technical + Behavioral)
- ðŸ”¹ **Tips Extraction** from Job Descriptions
- ðŸ”¹ **Resume Booster** suggestions based on Job Descriptions
- ðŸ”¹ **Practice Mode** mixing different question types
- ðŸ”¹ **CSV Export** of generated interview questions
- ðŸ”¹ **Responsive UI** built using Bootstrap
- ðŸ”¹ **Multi-Agent System**: Clean modular separation (QuestionAgent, TipsAgent, ResumeAgent)

---

## âš™ï¸ Technology Stack
- Python 3.x
- Flask
- Bootstrap 5
- Local LLM Server (Ollama, using Llama3 8B/13B)
- CSV Handling (Python `csv` module)

---

## ðŸ—ï¸ How to Run
1. Ensure Python 3.x is installed.
2. Install Flask:  
   ```
   pip install flask
   ```
3. Ensure Ollama is installed locally and running a model like Llama3.
4. Clone or download this repository.
5. Navigate to the project directory and run:
   ```
   python app.py
   ```
6. Open your browser and go to `http://localhost:5000/`

---

## ðŸ› ï¸ Solution Architecture Summary
- Frontend: User Browser (Bootstrap + Flask templates)
- Backend: Flask Web Server
- Logic Layer: QuestionAgent, TipsAgent, ResumeAgent
- LLM Inference Layer: Local Ollama LLM Server
- Optional: Export Module for CSV downloads

> User â†’ Flask Web Server â†’ Appropriate Agent â†’ Ollama LLM â†’ Result Rendered on Browser

---

## ðŸ“Š Percentage of Automation Across SDLC Phases
| SDLC Phase | Automation Achieved | Manual Effort Needed |
|:---|:---:|:---:|
| Requirement Gathering (Prompts Creation) | 90% | 10% |
| System Design (Architecture + Modeling) | 85% | 15% |
| Implementation (Web App, Agents, API Calls) | 95% | 5% |
| Testing (Manual Testing of Outputs) | 70% | 30% |
| Deployment (Local Server Only) | 80% | 20% |
| Maintenance & Updates | 70% | 30% |

**Overall Automation Achieved: â‰ˆ 85%** ðŸ”¥

---

## ðŸ§  Future Enhancements
- Integrate a login system for personalized practice sessions.
- Dockerize the solution for easy cloud deployment.
- Add Feedback Module to collect user evaluations on generated outputs.
- Integrate RAG (Retrieval-Augmented Generation) to enrich context dynamically.

---

## ðŸ‘¥ Credits
Developed by: **Geethika**  
Part of Final Project for:  
**COT6930 â€“ Generative AI and Software Development Lifecycles**

---
